{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Article Text Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# The low end of shared words to consider\n",
    "LOWER_BOUND = .20\n",
    "\n",
    "# The high end, since anything above this is probably SEO garbage or a\n",
    "# duplicate sentence\n",
    "UPPER_BOUND = .90\n",
    "\n",
    "def is_unimportant(word):\n",
    "    \"\"\"Decides if a word is ok to toss out for the sentence comparisons\"\"\"\n",
    "    return word in ['.', '!', ',', ] or '\\'' in word or word in stop_words\n",
    "\n",
    "def only_important(sent):\n",
    "    \"\"\"Just a little wrapper to filter on is_unimportant\"\"\"\n",
    "    return filter(lambda w: not is_unimportant(w), sent)\n",
    "\n",
    "def compare_sents(sent1, sent2):\n",
    "    \"\"\"Compare two word-tokenized sentences for shared words\"\"\"\n",
    "    if not len(sent1) or not len(sent2):\n",
    "        return 0\n",
    "    return len(set(only_important(sent1)) & set(only_important(sent2))) / ((len(sent1) + len(sent2)) / 2.0)\n",
    "\n",
    "def compare_sents_bounded(sent1, sent2):\n",
    "    \"\"\"If the result of compare_sents is not between LOWER_BOUND and\n",
    "    UPPER_BOUND, it returns 0 instead, so outliers don't mess with the sum\"\"\"\n",
    "    cmpd = compare_sents(sent1, sent2)\n",
    "    if LOWER_BOUND < cmpd < UPPER_BOUND:\n",
    "        return cmpd\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def compute_score(sent, sents):\n",
    "    \"\"\"Computes the average score of sent vs the other sentences (the result of\n",
    "    sent vs itself isn't counted because it's 1, and that's above\n",
    "    UPPER_BOUND)\"\"\"\n",
    "    if not len(sent):\n",
    "        return 0\n",
    "    return sum(compare_sents_bounded(sent, sent1) for sent1 in sents) / float(len(sents))\n",
    "\n",
    "def summarize(block):\n",
    "    \"\"\"Return the sentence that best summarizes block\"\"\"\n",
    "    if not block:\n",
    "        return None\n",
    "    sents = nltk.sent_tokenize(block)\n",
    "    word_sents = list(map(nltk.word_tokenize, sents))\n",
    "    d = dict((compute_score(word_sent, word_sents), sent)\n",
    "             for sent, word_sent in zip(sents, word_sents))\n",
    "    return d[max(d.keys())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'However, taking supplemental vitamin C 500 mg daily without antihypertensives does not seem to reduce systolic or diastolic blood pressure .'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article = \"\"\"\n",
    "Taking vitamin C orally along with conventional antihypertensive medications appears to modestly decrease systolic blood pressure, but the effects on diastolic pressure are mixed . In patients with type 2 diabetes, taking vitamin C 500 mg daily for 4 weeks, in addition to antihypertensives, seems to reduce arterial blood pressure and decrease arterial stiffness . However, taking supplemental vitamin C 500 mg daily without antihypertensives does not seem to reduce systolic or diastolic blood pressure . Dietary restriction of vitamin C is associated with increases in both diastolic and systolic blood pressure (10354\n",
    "\"\"\"\n",
    "summarize(article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the Text ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from neo4j.v1 import GraphDatabase, basic_auth\n",
    "\n",
    "driver = GraphDatabase.driver(\"bolt://localhost:7687\", auth=basic_auth(\"neo4j\", \"naturalmed\"))\n",
    "session = driver.session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query = session.run(\"MATCH (m:Medicine)-[r1]->(n:DiseaseInteraction)-[r2]->(d:Disease) \\\n",
    "                     RETURN m.name, n.text, d.id\")\n",
    "\n",
    "data = [(r['m.name'], r['n.text'], r['d.id'], summarize(r['n.text'])) for r in query]\n",
    "\n",
    "infos = [e[1] for e in data]\n",
    "summaries = [e[3] for e in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Constituents isolated from Acacia rigidula are thought to have stimulant effects . Additionally, dietary supplements listing Acacia rigidula as an ingredient have been shown to contain the stimulant beta-methylphenethylamine (BMPEA) and greater amounts of other stimulant compounds than would normally be found in plant parts . BMPEA has been shown to increase blood pressure and heart rate in animals . Theoretically, taking Acacia rigidula supplements might exacerbate high blood pressure.'"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infos[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Theoretically, taking Acacia rigidula supplements might exacerbate high blood pressure.'"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2336\n",
      "2336\n"
     ]
    }
   ],
   "source": [
    "print(len(infos))\n",
    "print(len(summaries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_vocab(lst):\n",
    "    \"\"\" Returns the vocab and count of each word in a list of strings\n",
    "    \"\"\"\n",
    "    d = Counter(w.lower() for txt in lst for w in txt.split())\n",
    "    return list(d.keys()), list(d.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab, count = get_vocab(infos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['acacia', 'can', 'cause', 'reactions', 'in', 'individuals', 'allergic']"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4657\n",
      "4657\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab))\n",
    "print(len(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "empty = 0 # RNN mask of no data\n",
    "eos = 1  # end of sentence\n",
    "start_idx = eos + 1 # first real word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_idx(vocab, vocabcount):\n",
    "    word2idx = dict((word, idx+start_idx) for idx,word in enumerate(vocab))\n",
    "    word2idx['<empty>'] = empty\n",
    "    word2idx['<eos>'] = eos\n",
    "    \n",
    "    idx2word = dict((idx,word) for word,idx in word2idx.items())\n",
    "\n",
    "    return word2idx, idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word2idx, idx2word = get_idx(vocab, count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and formating the GloVe lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_dim = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "glove_path ='./../Datasets/glove.6B/glove.6B.%sd.txt' % embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_n_symbols = !wc -l {glove_path}\n",
    "glove_n_symbols = int(glove_n_symbols[0].split()[0])\n",
    "glove_n_symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "glove_index_dict = {}\n",
    "glove_embedding_weights = np.empty((glove_n_symbols, embedding_dim))\n",
    "globale_scale = 0.1\n",
    "\n",
    "with open(glove_path, 'r') as fp:\n",
    "    i = 0\n",
    "    for l in fp:\n",
    "        l = l.strip().split()\n",
    "        w = l[0]\n",
    "        glove_index_dict[w] = i\n",
    "        glove_embedding_weights[i,:] = [float(e) for e in l[1:]]\n",
    "        i += 1\n",
    "\n",
    "glove_embedding_weights *= globale_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.0418     0.024968  -0.041242  ..., -0.018411  -0.011514  -0.078581 ]\n",
      " [ 0.0013441  0.023682  -0.016899  ..., -0.056657   0.0044691  0.030392 ]\n",
      " [ 0.015164   0.030177  -0.016763  ..., -0.035652   0.0016413  0.010216 ]\n",
      " ..., \n",
      " [-0.051181   0.0058706  0.10913   ..., -0.025003  -0.1125     0.15863  ]\n",
      " [-0.075898  -0.047426   0.04737   ...,  0.078954  -0.0014116  0.06448  ]\n",
      " [ 0.0072617 -0.051393   0.04728   ..., -0.018907  -0.059021   0.055559 ]]\n"
     ]
    }
   ],
   "source": [
    "print(glove_embedding_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.064410429768131147"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_embedding_weights.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for w,i in glove_index_dict.items():\n",
    "    w = w.lower()\n",
    "    if w not in glove_index_dict:\n",
    "        glove_index_dict[w] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seed = 42\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "np.random.seed(seed)\n",
    "\n",
    "shape = (vocab_size, embedding_dim)\n",
    "scale = glove_embedding_weights.std() * np.sqrt(12) / 2\n",
    "embedding = np.random.uniform(low=-scale, high=scale, size=shape)\n",
    "\n",
    "c = 0\n",
    "for i in range(len(vocab)):\n",
    "    w = idx2word[i]\n",
    "    g = glove_index_dict.get(w, glove_index_dict.get(w.lower()))\n",
    "    if g is None and w.startswith('#'): # glove has no hastags (I think...)\n",
    "        w = w[1:]\n",
    "        g = glove_index_dict.get(w, glove_index_dict.get(w.lower()))\n",
    "    if g is not None:\n",
    "        embedding[i,:] = glove_embedding_weights[g,:]\n",
    "        c+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4657"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4657"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "glove_thr = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2glove = {}\n",
    "for w in word2idx:\n",
    "    if w in glove_index_dict:\n",
    "        g = w\n",
    "    elif w.lower() in glove_index_dict:\n",
    "        g = w.lower()\n",
    "    elif w.startswith('#') and w[1:] in glove_index_dict:\n",
    "        g = w[1:]\n",
    "    elif w.startswith('#') and w[1:].lower() in glove_index_dict:\n",
    "        g = w[1:].lower()\n",
    "    else:\n",
    "        continue\n",
    "    word2glove[w] = g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66 of glove substitutes found\n"
     ]
    }
   ],
   "source": [
    "normed_embedding = embedding/np.array([np.sqrt(np.dot(gweight,gweight)) for gweight in embedding])[:,None]\n",
    "\n",
    "nb_unknown_words = 100\n",
    "\n",
    "glove_match = []\n",
    "for w,idx in word2idx.items():\n",
    "    if idx >= vocab_size-nb_unknown_words and w.isalpha() and w in word2glove:\n",
    "        gidx = glove_index_dict[word2glove[w]]\n",
    "        gweight = glove_embedding_weights[gidx,:].copy()\n",
    "        # find row in embedding that has the highest cos score with gweight\n",
    "        gweight /= np.sqrt(np.dot(gweight,gweight))\n",
    "        score = np.dot(normed_embedding[:vocab_size-nb_unknown_words], gweight)\n",
    "        while True:\n",
    "            embedding_idx = score.argmax()\n",
    "            s = score[embedding_idx]\n",
    "            if s < glove_thr:\n",
    "                break\n",
    "            if idx2word[embedding_idx] in word2glove :\n",
    "                glove_match.append((w, embedding_idx, s)) \n",
    "                break\n",
    "            score[embedding_idx] = -1\n",
    "glove_match.sort(key = lambda x: -x[2])\n",
    "print('{} of glove substitutes found'.format(len(glove_match)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.610201867614 dock => harbor\n",
      "0.582583655227 mate => younger\n",
      "0.574584160008 habitually => habitual\n",
      "0.571254292226 wahoo => pud\n",
      "0.558218106953 ra => haw\n",
      "0.556304544124 enlargement => brussels\n",
      "0.532109899224 wallflower => dendrobium\n",
      "0.52087705412 mansa => terminalia\n",
      "0.51769653988 yerba => breadfruit\n",
      "0.502346659661 yarrow => wort\n"
     ]
    }
   ],
   "source": [
    "for orig, sub, score in glove_match[-10:]:\n",
    "    print(score, orig,'=>', idx2word[sub])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "glove_idx2idx = dict((word2idx[w],embedding_idx) for  w, embedding_idx, _ in glove_match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2336"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = [[word2idx[token.lower()] for token in summary.split()] for summary in summaries]\n",
    "len(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   9.,  257.,   10.,   26.,   49.,  112.,  131.,  141.,  152.,\n",
       "         167.,  127.,  129.,  140.,  182.,  118.,   98.,    0.,   92.,\n",
       "          62.,   52.,   39.,   37.,   35.,   35.,   27.,   21.,   12.,\n",
       "          16.,    9.,   11.,    9.,    4.,    2.,    0.,    3.,    3.,\n",
       "           1.,    0.,    1.,    3.,    1.,    0.,    3.,    1.,    1.,\n",
       "           2.,    0.,    0.,    3.,    3.]),\n",
       " array([  1.  ,   1.94,   2.88,   3.82,   4.76,   5.7 ,   6.64,   7.58,\n",
       "          8.52,   9.46,  10.4 ,  11.34,  12.28,  13.22,  14.16,  15.1 ,\n",
       "         16.04,  16.98,  17.92,  18.86,  19.8 ,  20.74,  21.68,  22.62,\n",
       "         23.56,  24.5 ,  25.44,  26.38,  27.32,  28.26,  29.2 ,  30.14,\n",
       "         31.08,  32.02,  32.96,  33.9 ,  34.84,  35.78,  36.72,  37.66,\n",
       "         38.6 ,  39.54,  40.48,  41.42,  42.36,  43.3 ,  44.24,  45.18,\n",
       "         46.12,  47.06,  48.  ]),\n",
       " <a list of 50 Patch objects>)"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXsAAAD8CAYAAACW/ATfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ne0lqwP8BecswUeQ7zxQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3d29127198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist([len(y) for y in Y], bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2336"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = [[word2idx[token.lower()] for token in info.split()] for info in infos]\n",
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 266.,   53.,  161.,  150.,  168.,  168.,  144.,  180.,  155.,\n",
       "         123.,  197.,  116.,   85.,   70.,   48.,   41.,   44.,   25.,\n",
       "          18.,   14.,   18.,   11.,    5.,    6.,    9.,    2.,   13.,\n",
       "           1.,    8.,    3.,    5.,    3.,    1.,    1.,    1.,    3.,\n",
       "           1.,    0.,    1.,    4.,    4.,    0.,    0.,    2.,    2.,\n",
       "           1.,    2.,    1.,    1.,    1.]),\n",
       " array([   1. ,    4.1,    7.2,   10.3,   13.4,   16.5,   19.6,   22.7,\n",
       "          25.8,   28.9,   32. ,   35.1,   38.2,   41.3,   44.4,   47.5,\n",
       "          50.6,   53.7,   56.8,   59.9,   63. ,   66.1,   69.2,   72.3,\n",
       "          75.4,   78.5,   81.6,   84.7,   87.8,   90.9,   94. ,   97.1,\n",
       "         100.2,  103.3,  106.4,  109.5,  112.6,  115.7,  118.8,  121.9,\n",
       "         125. ,  128.1,  131.2,  134.3,  137.4,  140.5,  143.6,  146.7,\n",
       "         149.8,  152.9,  156. ]),\n",
       " <a list of 50 Patch objects>)"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3d28f25c18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist([len(x) for x in X], bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dumping the objects to the disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "FN = 'vocabulary-embedding'\n",
    "\n",
    "with open('./../Dumps/%s.pkl'%FN,'wb') as fp:\n",
    "    pickle.dump((embedding, idx2word, word2idx, glove_idx2idx),fp,-1)\n",
    "\n",
    "with open('./../Dumps/%s.data.pkl'%FN,'wb') as fp:\n",
    "    pickle.dump((X,Y),fp,-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "maxlend = 25 # 0 - if we dont want to use description at all\n",
    "maxlenh = 25\n",
    "maxlen = maxlend + maxlenh\n",
    "rnn_size = 512 # must be same as 160330-word-gen\n",
    "rnn_layers = 3  # match FN1\n",
    "batch_norm = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "activation_rnn_size = 40 if maxlend else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# training parameters\n",
    "seed = 42\n",
    "p_W, p_U, p_dense, p_emb, weight_decay = 0, 0, 0, 0, 0\n",
    "optimizer = 'adam'\n",
    "LR = 1e-4\n",
    "batch_size = 64\n",
    "nflips = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_train_samples = 1500\n",
    "nb_val_samples = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('./../Dumps/%s.pkl'%FN, 'rb') as fp:\n",
    "    embedding, idx2word, word2idx, glove_idx2idx = pickle.load(fp)\n",
    "vocab_size, embedding_size = embedding.shape\n",
    "\n",
    "with open('./../Dumps/%s.data.pkl'%FN, 'rb') as fp:\n",
    "    X, Y = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_unknown_words = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples 2336 2336\n",
      "dimension of embedding space for words 50\n",
      "vocabulary size 4657 the last 10 words can be used as place holders for unknown/oov words\n",
      "total number of different words 4659 4659\n",
      "number of words outside vocabulary which we can substitue using glove similarity 66\n",
      "number of words that will be regarded as unknonw(unk)/out-of-vocabulary(oov) -64\n"
     ]
    }
   ],
   "source": [
    "print('number of examples',len(X),len(Y))\n",
    "print('dimension of embedding space for words',embedding_size)\n",
    "print('vocabulary size', vocab_size, 'the last %d words can be used as place holders for unknown/oov words'%nb_unknown_words)\n",
    "print('total number of different words',len(idx2word), len(word2idx))\n",
    "print('number of words outside vocabulary which we can substitue using glove similarity', len(glove_idx2idx))\n",
    "print('number of words that will be regarded as unknonw(unk)/out-of-vocabulary(oov)',len(idx2word)-vocab_size-len(glove_idx2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(nb_unknown_words):\n",
    "    idx2word[vocab_size-1-i] = '<%d>'%i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "oov0 = vocab_size-nb_unknown_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(oov0, len(idx2word)):\n",
    "    idx2word[i] = idx2word[i]+'^'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1836, 1836, 500, 500)"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=nb_val_samples, random_state=seed)\n",
    "len(X_train), len(Y_train), len(X_test), len(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del X\n",
    "del Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "empty = 0\n",
    "eos = 1\n",
    "idx2word[empty] = '_'\n",
    "idx2word[eos] = '~'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "import random, sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def prt(label, x):\n",
    "    print(label+':')\n",
    "    s = \"\"\n",
    "    for w in x:\n",
    "        s += \" \" + idx2word[w]\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H:\n",
      " animal research suggests that, when administered intravenously, paba can have anticoagulant activity .\n",
      "D:\n",
      " animal research suggests that, when administered intravenously, paba can have anticoagulant activity . theoretically, use of paba might increase the risk of bleeding in patients with bleeding disorders.\n"
     ]
    }
   ],
   "source": [
    "i = 3\n",
    "prt('H',Y_train[i])\n",
    "prt('D',X_train[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras Model Time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout, RepeatVector\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# seed weight initialization\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "regularizer = l2(weight_decay) if weight_decay else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/diego/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:5: UserWarning: The `dropout` argument is no longer support in `Embedding`. You can apply a `keras.layers.SpatialDropout1D` layer right after the `Embedding` layer to get the same behavior.\n",
      "/home/diego/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:5: UserWarning: Update your `Embedding` call to the Keras 2 API: `Embedding(4657, 50, input_length=50, weights=[array([[-..., mask_zero=True, name=\"embedding_1\", embeddings_regularizer=None)`\n",
      "/home/diego/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:10: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(512, return_sequences=True, name=\"lstm_1\", kernel_regularizer=None, bias_regularizer=None, recurrent_regularizer=None, dropout=0, recurrent_dropout=0)`\n",
      "/home/diego/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:10: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(512, return_sequences=True, name=\"lstm_2\", kernel_regularizer=None, bias_regularizer=None, recurrent_regularizer=None, dropout=0, recurrent_dropout=0)`\n",
      "/home/diego/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:10: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(512, return_sequences=True, name=\"lstm_3\", kernel_regularizer=None, bias_regularizer=None, recurrent_regularizer=None, dropout=0, recurrent_dropout=0)`\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_size,\n",
    "                    input_length=maxlen,\n",
    "                    W_regularizer=regularizer, dropout=p_emb, weights=[embedding], mask_zero=True,\n",
    "                    name='embedding_1'))\n",
    "for i in range(rnn_layers):\n",
    "    lstm = LSTM(rnn_size, return_sequences=True, # batch_norm=batch_norm,\n",
    "                W_regularizer=regularizer, U_regularizer=regularizer,\n",
    "                b_regularizer=regularizer, dropout_W=p_W, dropout_U=p_U,\n",
    "                name='lstm_%d'%(i+1)\n",
    "                  )\n",
    "    model.add(lstm)\n",
    "    model.add(Dropout(p_dense,name='dropout_%d'%(i+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers.core import Lambda\n",
    "import keras.backend as K\n",
    "\n",
    "def simple_context(X, mask, n=activation_rnn_size, maxlend=maxlend, maxlenh=maxlenh):\n",
    "    desc, head = X[:,:maxlend,:], X[:,maxlend:,:]\n",
    "    head_activations, head_words = head[:,:,:n], head[:,:,n:]\n",
    "    desc_activations, desc_words = desc[:,:,:n], desc[:,:,n:]\n",
    "    \n",
    "    # RTFM http://deeplearning.net/software/theano/library/tensor/basic.html#theano.tensor.batched_tensordot\n",
    "    # activation for every head word and every desc word\n",
    "    activation_energies = K.batch_dot(head_activations, desc_activations, axes=(2,2))\n",
    "    # make sure we dont use description words that are masked out\n",
    "    activation_energies = activation_energies + -1e20*K.expand_dims(1.-K.cast(mask[:, :maxlend],'float32'),1)\n",
    "    \n",
    "    # for every head word compute weights for every desc word\n",
    "    activation_energies = K.reshape(activation_energies,(-1,maxlend))\n",
    "    activation_weights = K.softmax(activation_energies)\n",
    "    activation_weights = K.reshape(activation_weights,(-1,maxlenh,maxlend))\n",
    "\n",
    "    # for every head word compute weighted average of desc words\n",
    "    desc_avg_word = K.batch_dot(activation_weights, desc_words, axes=(2,1))\n",
    "    return K.concatenate((desc_avg_word, head_words))\n",
    "\n",
    "\n",
    "class SimpleContext(Lambda):\n",
    "    def __init__(self,**kwargs):\n",
    "        super(SimpleContext, self).__init__(simple_context,**kwargs)\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return input_mask[:, maxlend:]\n",
    "    \n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        nb_samples = input_shape[0]\n",
    "        n = 2*(rnn_size - activation_rnn_size)\n",
    "        return (nb_samples, maxlenh, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-260-149a2088cdfb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mactivation_rnn_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSimpleContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'simplecontext_1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m model.add(TimeDistributed(Dense(vocab_size,\n\u001b[1;32m      4\u001b[0m                                 \u001b[0mW_regularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mregularizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_regularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mregularizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                 name = 'timedistributed_1')))\n",
      "\u001b[0;32m/home/diego/anaconda3/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    453\u001b[0m                           output_shapes=[self.outputs[0]._keras_shape])\n\u001b[1;32m    454\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 455\u001b[0;31m             \u001b[0moutput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m                 raise TypeError('All layers in a Sequential model '\n",
      "\u001b[0;32m/home/diego/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    557\u001b[0m             \u001b[0;31m# Infering the output shape is only relevant for Theano.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_to_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 559\u001b[0;31m                 \u001b[0moutput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_output_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    560\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/diego/anaconda3/lib/python3.6/site-packages/keras/layers/core.py\u001b[0m in \u001b[0;36mcompute_output_shape\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    625\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m                     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m                     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    628\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_elem\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx_elem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/diego/anaconda3/lib/python3.6/site-packages/keras/layers/core.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, mask)\u001b[0m\n\u001b[1;32m    657\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'mask'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marg_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m             \u001b[0marguments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mask'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 659\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0marguments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcompute_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-259-6cb879a16b11>\u001b[0m in \u001b[0;36msimple_context\u001b[0;34m(X, mask, n, maxlend, maxlenh)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mactivation_energies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_activations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc_activations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# make sure we dont use description words that are masked out\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mactivation_energies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivation_energies\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1e20\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mmaxlend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# for every head word compute weights for every desc word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "if activation_rnn_size:\n",
    "    model.add(SimpleContext(name='simplecontext_1'))\n",
    "model.add(TimeDistributed(Dense(vocab_size,\n",
    "                                W_regularizer=regularizer, b_regularizer=regularizer,\n",
    "                                name = 'timedistributed_1')))\n",
    "model.add(Activation('softmax', name='activation_1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#session.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
